---
title: "Emily_ML"
format: html
editor: visual
---

All necessary code for dashboard in this chunk
```{r}
library(readxl)
library(dplyr)
library(tidymodels)
library(beans)
library(doParallel)
library(bundle)
library(dplyr)
library(callr)
library(stacks)
library(tictoc)
library(xgboost)

# set-up
set.seed(250)
bean_split = initial_split(beans, prop = 0.8, strata = class)
bean_training <- training(bean_split)
bean_testing <- testing(bean_split)

bean_rec <- training(bean_split) %>%
  recipe(class ~.) %>% 
  step_normalize(all_predictors())

load("RF_predict.RData")
model_rf_object <- unbundle(rf_bundle)

load("XG_predict.RData")
model_xg_object <- unbundle(xg_bundle)

load("RF_XG_predict.RData")
model_ensemble_object <- unbundle(test)

load("RF_XG_Lasso_predict.RData")
model_ensemble_object_3 <- unbundle(model_ensemble_object)

load("Lasso_PCA_predict.RData")
model_Lasso_PCA_object <- unbundle(lasso_PCA_bundle)

# largest ensemble model right now
load("RF_XG_Lasso_LassoPCA_predict.RData")
ensemble_object_4 <- unbundle(model_ensemble_object_4)

set.seed(as.numeric(Sys.time()) %% 1e6)
sample_test = sample_n(bean_testing, 5)
sample_test
predict(model_rf_object, sample_test)
predict(model_xg_object, sample_test)
predict(model_ensemble_object, sample_test)
predict(model_ensemble_object_3, sample_test)
predict(model_Lasso_PCA_object, sample_test)
predict(ensemble_object_4, sample_test)
```

______________________________

```{r}
library(readxl)
library(dplyr)
library(tidymodels)
library(beans)
library(doParallel)
library(bundle)
library(dplyr)
library(callr)
library(stacks)

# set-up
set.seed(250)
bean_split = initial_split(beans, prop = 0.8, strata = class)
bean_training <- training(bean_split)
bean_testing <- testing(bean_split)

folds <- rsample::vfold_cv(bean_training, v = 5)

bean_rec <- training(bean_split) %>%
  recipe(class ~.) %>% 
  step_normalize(all_predictors())

bean_wflow <- 
  workflow() %>% 
  add_recipe(bean_rec)
```

## Random Forest CV

```{r, eval = FALSE}
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Define control for grid search
ctrl_grid <- control_grid(save_pred = TRUE, parallel_over = "resamples")

# Specify the random forest model
rand_forest_spec <- 
  rand_forest(
    mtry = tune(),         # Number of predictors to sample at each split
    min_n = tune(),        # Minimum number of observations per node
    trees = 500            # Fixed number of trees
  ) %>%
  set_mode("classification") %>%
  set_engine("ranger") 

# Define the workflow
rand_forest_wflow <- 
  bean_wflow %>%          
  add_model(rand_forest_spec)

set.seed(5)
# Tune hyperparameters using grid search
rand_forest_res <- 
  tune_grid(
    object = rand_forest_wflow, 
    resamples = folds,    # Cross-validation folds
    grid = 5,            # Number of grid combinations
    control = ctrl_grid   # Use parallel processing
  )

# Save and load results for reproducibility
saveRDS(rand_forest_res, "rand_forest_res.rds")
rand_forest_res <- readRDS("rand_forest_res.rds")

rand_forest_res %>% select_best() # mtry = 7, min_n = 26

# Stop parallel processing
stopCluster(cl)
```

## XGBoost CV

```{r, eval = FALSE}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

ctrl_grid <- control_grid(save_pred = TRUE, parallel_over = "resamples")

xgboost_spec <-
  boost_tree(trees = 500, tree_depth = tune(), learn_rate = tune(), loss_reduction = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgboost_wflow <- 
  bean_wflow %>%
  add_model(xgboost_spec) %>%
  update_recipe(bean_rec)

set.seed(5)
xgboost_res <-
  tune_grid(
    object = xgboost_wflow, 
    resamples = folds, 
    grid = 5,
    control = ctrl_grid
  )

saveRDS(xgboost_res, "xgboost_res.rds")
xgboost_res <- readRDS("xgboost_res.rds")

stopCluster(cl)

xgboost_res %>% select_best()
# tree_depth = 2, learn_rate = 0.0869, loss_reduction = 0.0147
```

## Single models: RF and XGBoost
```{r}
# specify recipe / apply to training data
bean_rec <- training(bean_split) %>%
  recipe(class ~.) %>% 
  step_normalize(all_predictors())

final_rf_wf <- workflow() %>%
  add_recipe(bean_rec) %>%
  add_model(final_rf_mod)

rf_mod = final_rf_wf %>%  
  fit(bean_training)

# rf_bundle <- bundle(rf_mod)
# save(rf_bundle, file = "RF_predict.RData")

final_xgb_wf <- workflow() %>%
  add_recipe(bean_rec) %>%
  add_model(final_xgb_model)

xg_mod = final_xgb_wf %>%  
  fit(bean_training)

# xg_bundle <- bundle(xg_mod)
# save(xg_bundle, file = "XG_predict.RData")
```

## Ensemble Model for Random Forest and XGBoost (using only best models due to computational constraints)

```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

final_rf_mod <- rand_forest(mtry = 7,
                         trees = 500,
                         min_n = 26) %>%
  set_mode("classification") %>%
  set_engine("ranger") 

final_xgb_model <- boost_tree(tree_depth = 2,
                         trees = 500,
                         learn_rate = 0.0869,
                      loss_reduction = 0.0147) %>%
  set_mode("classification")%>%
  set_engine("xgboost")

untrained_recipe <- recipe(class ~., data = bean_training) %>% 
  step_normalize(all_predictors())

# Define the resampling strategy
set.seed(123)
df_folds <- vfold_cv(bean_training, v = 5)

# Define the workflow for each model
rf_workflow <- workflow() %>%
  add_model(final_rf_mod) %>%
  add_recipe(untrained_recipe)

xgb_workflow <- workflow() %>%
  add_model(final_xgb_model) %>%
  add_recipe(untrained_recipe)

# Fit the models using resampling with control settings for stacking
rf_res <- fit_resamples(rf_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))
xgb_res <- fit_resamples(xgb_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))

# Create a model stack
model_stack <- stacks() %>%
  add_candidates(rf_res) %>%
  add_candidates(xgb_res) %>% 
  blend_predictions() 

# test = model_stack %>% fit_members() %>% butcher() %>% bundle()
# save(test, file = "RF_XG_predict.RData")
```


## Prepping Bella's final models for collective ensemble

## Single model: Lasso (no PCA)
```{r}
bean_rec <- training(bean_split) %>%
  recipe(class ~.) %>% 
  step_normalize(all_predictors())

final_lasso_mod = multinom_reg(penalty = 0.00005179475, mixture = 1) %>%
  set_engine("nnet") %>% 
  set_mode("classification")

final_lasso_wf = workflow() %>% 
  add_recipe(bean_rec) %>% 
  add_model(final_lasso_mod)

lasso_mod = final_lasso_wf %>%  
  fit(bean_training)

# lasso_bundle <- bundle(lasso_mod)
# save(lasso_bundle, file = "Lasso_predict.RData")
```

## Single model: Lasso with PCA
```{r}
pca_recipe = bean_recipe %>% step_pca(all_predictors())

final_lasso_PCA_model = multinom_reg(penalty = 0.0003393222, mixture = 1) %>%
  set_engine("nnet") %>% 
  set_mode("classification")

final_lasso_PCA_wf = workflow() %>% 
  add_recipe(pca_recipe) %>% 
  add_model(final_lasso_PCA_model)

lasso_PCA_mod = final_lasso_PCA_wf %>%  
  fit(bean_training)

# lasso_PCA_bundle <- bundle(lasso_PCA_mod)
# save(lasso_PCA_bundle, file = "Lasso_PCA_predict.RData")
```


## Collective Ensemble Model (Random Forest, XGBoost, Lasso, Lasso with PCA first)
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

final_rf_mod <- rand_forest(mtry = 7,
                         trees = 500,
                         min_n = 26) %>%
  set_mode("classification") %>%
  set_engine("ranger") 

final_xgb_model <- boost_tree(tree_depth = 2,
                         trees = 500,
                         learn_rate = 0.0869,
                      loss_reduction = 0.0147) %>%
  set_mode("classification")%>%
  set_engine("xgboost")

final_lasso_model = multinom_reg(penalty = 0.00005179475, mixture = 1) %>%
  set_engine("nnet") %>% 
  set_mode("classification")

final_lasso_PCA_model = multinom_reg(penalty = 0.0003393222, mixture = 1) %>%
  set_engine("nnet") %>% 
  set_mode("classification")

untrained_recipe <- recipe(class ~., data = bean_training) %>% 
  step_normalize(all_predictors())

untrained_recipe_PCA <- recipe(class ~., data = bean_training) %>% 
  step_pca(all_predictors())

# Define the resampling strategy
set.seed(123)
df_folds <- vfold_cv(bean_training, v = 5)

# Define the workflow for each model
rf_workflow <- workflow() %>%
  add_model(final_rf_mod) %>%
  add_recipe(untrained_recipe)

xgb_workflow <- workflow() %>%
  add_model(final_xgb_model) %>%
  add_recipe(untrained_recipe)

lasso_workflow <- workflow() %>%
  add_model(final_lasso_model) %>%
  add_recipe(untrained_recipe_PCA)

lasso_PCA_workflow <- workflow() %>%
  add_model(final_lasso_PCA_model) %>%
  add_recipe(untrained_recipe)


# Fit the models using resampling with control settings for stacking
rf_res <- fit_resamples(rf_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))
xgb_res <- fit_resamples(xgb_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))
lasso_res <- fit_resamples(lasso_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))
lasso_PCA_res <- fit_resamples(lasso_PCA_workflow, resamples = df_folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE))

# Create a model stack
model_stack_4 <- stacks() %>%
  add_candidates(rf_res) %>%
  add_candidates(xgb_res) %>% 
  add_candidates(lasso_res) %>% 
  add_candidates(lasso_PCA_res) %>%  
  blend_predictions()

# ensemble_object_4 = model_stack_4 %>% fit_members()
# model_ensemble_object_4 = ensemble_object_4 %>% bundle()
# save(model_ensemble_object_4, file = "RF_XG_Lasso_LassoPCA_predict.RData")
```
